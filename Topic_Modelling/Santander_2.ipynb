{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de las librerías a utilizar#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aquí estamos preparando las bases o datos que se sacaron del webscrapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'C:\\Users\\danie\\OneDrive\\Escritorio\\Data Science\\Santander_Project\\Salida.xlsx'\n",
    "\n",
    "df = pd.read_excel(file)\n",
    "#df.dropna(inplace=True)\n",
    "\n",
    "df[df['Rate'] != 3] #No lo tomé porque hay opinion divididas en un rating del 3\n",
    "\n",
    "df['Pos_Bool'] = np.where(df['Rate'] > 3, 1, 0)\n",
    "df_pos = df[df['Pos_Bool'] == 1]\n",
    "df_neg = df[df['Pos_Bool'] == 0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model: Aquí finalmente aplicamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"trabajo\" + 0.010*\"ambiente\" + 0.009*\"experiencia\" + 0.009*\"empresa\" + 0.008*\"beneficios\" + 0.007*\"clientes\" + 0.007*\"persona\"'),\n",
       " (5,\n",
       "  '0.030*\"trabajo\" + 0.021*\"ma\" + 0.015*\"banco\" + 0.013*\"gusto\" + 0.012*\"trabajar\" + 0.008*\"ambiente\" + 0.008*\"equipo\"'),\n",
       " (4,\n",
       "  '0.011*\"trabajo\" + 0.010*\"crecimiento\" + 0.007*\"and\" + 0.007*\"to\" + 0.006*\"seguridad\" + 0.006*\"nivel\" + 0.006*\"banco\"'),\n",
       " (1,\n",
       "  '0.036*\"laboral\" + 0.027*\"ambiente\" + 0.026*\"buen\" + 0.022*\"empresa\" + 0.017*\"trabajo\" + 0.016*\"buena\" + 0.010*\"experiencia\"'),\n",
       " (2,\n",
       "  '0.007*\"creditos\" + 0.007*\"productos\" + 0.007*\"banco\" + 0.006*\"etc\" + 0.006*\"años\" + 0.005*\"desarrollo\" + 0.005*\"solo\"')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# compile sample documents into a list\n",
    "set = [com for com in df_pos['Comentario']]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in spanish_stopwords]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [wnl.lemmatize(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "    \n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=6, id2word = dictionary, passes=20)\n",
    "ldamodel.print_topics(num_topics=5, num_words=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.045*\"trabajo\" + 0.045*\"implica\" + 0.044*\"bastante\" + 0.043*\"cuidado\" + 0.042*\"hacemos\" + 0.042*\"puede\" + 0.042*\"organismos\"'), (1, '0.076*\"bastante\" + 0.074*\"trabajo\" + 0.044*\"organismos\" + 0.044*\"economicas\" + 0.042*\"trabajar\" + 0.042*\"menos\" + 0.041*\"cuidado\"'), (2, '0.080*\"trabajo\" + 0.077*\"bastante\" + 0.050*\"mal\" + 0.046*\"implica\" + 0.042*\"tener\" + 0.040*\"valorizacion\" + 0.040*\"general\"')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "wnl = WordNetLemmatizer()\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "\n",
    "\n",
    "# compile sample documents into a list\n",
    "set_neg = [com for com in df_neg['Comentario']]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts_neg = []\n",
    "\n",
    "# loop through document list\n",
    "for i in set_neg:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw_n = i.lower()\n",
    "    tokens_n = tokenizer.tokenize(raw_n)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens_n = [i for i in tokens if not i in spanish_stopwords]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens_n = [wnl.lemmatize(i) for i in stopped_tokens_n]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts_neg.append(stemmed_tokens_n)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary_n = corpora.Dictionary(texts_neg)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus_n = [dictionary_n.doc2bow(text) for text in texts_neg]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel_pos = gensim.models.ldamodel.LdaModel(corpus_n, num_topics=3, id2word = dictionary_n, passes=20)\n",
    "print(ldamodel_pos.print_topics(num_topics=3, num_words=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
